{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Woj_Shams_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diversifyguy/ML_Apps/blob/main/Woj_Shams_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cGQeDdfUvlJ"
      },
      "source": [
        "#hide\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2VVSB1ZUvlK"
      },
      "source": [
        "#hide\n",
        "from fastbook import *\n",
        "from IPython.display import display,HTML"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPhxqBhfUvlL"
      },
      "source": [
        "# Woj vs. Shams - Twitter Analysis using NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WCLAv39UvlL"
      },
      "source": [
        "Anyone who seriously follows the NBA knows where to go for breaking news: the Twitter accounts of ESPN's Adrian Wojnarowski and/or The Athletic's Shams Charania.\n",
        "\n",
        "Like any Twitter account, it's easy to forget that Woj' and Shams' accounts both grew from humble beginnings.\n",
        "\n",
        "Here's Woj's first ever tweet from @wojespn (note: Woj's Twitter handle changed when he joined ESPN in 2009), dated 24-June-2009: \n",
        "\n",
        "> twitter: https://twitter.com/wojespn/status/2311135902\n",
        "\n",
        "Since then, Woj's follower count has grown to 4.7 million as of 6-July-2021.\n",
        "\n",
        "Here's Shams's first ever tweet, dated 15-August-2010: \n",
        "\n",
        ">twitter: https://twitter.com/ShamsCharania/status/21240227201\n",
        "\n",
        "Since then, Shams' follower count has grown to 1.2 million as of 6-July-2021.\n",
        "\n",
        "What might these accounts have to teach us about the NBA and sports news media? The purpose of this exercise is to find out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSjwTI3OUvlM"
      },
      "source": [
        "### Visualizing the Tweets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwSwCjyZV3Bi"
      },
      "source": [
        "After using [twint](https://pypi.org/project/twint/) to extract all the tweets from Woj' and Shams' accounts, I went about cleaning the data and organizing it into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieUik1l7fkaR"
      },
      "source": [
        "#hide\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xagnq5j5mDvg"
      },
      "source": [
        "# pandas to read our csv file\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p65r0WpHxg5i"
      },
      "source": [
        "#hide\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW2ADo37Q2Mc"
      },
      "source": [
        "df = pd.read_csv('/content/woj.csv', sep='\\t', lineterminator='\\r')\n",
        "# df2 = pd.read_csv('/content/shams.csv', sep='\\t', lineterminator='\\r')\n",
        "# frames = [df1, df2]\n",
        "# df = pd.concat(frames)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "lSxdkcDB2hoH",
        "outputId": "d28cd9a0-277c-4105-f1a5-7057be71256f"
      },
      "source": [
        "display(df.info())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16539 entries, 0 to 16538\n",
            "Data columns (total 36 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id               16539 non-null  object \n",
            " 1   conversation_id  16538 non-null  float64\n",
            " 2   created_at       16538 non-null  object \n",
            " 3   date             16538 non-null  object \n",
            " 4   time             16538 non-null  object \n",
            " 5   timezone         16538 non-null  float64\n",
            " 6   user_id          16538 non-null  float64\n",
            " 7   username         16538 non-null  object \n",
            " 8   name             16538 non-null  object \n",
            " 9   place            0 non-null      float64\n",
            " 10  tweet            16538 non-null  object \n",
            " 11  language         16538 non-null  object \n",
            " 12  mentions         16538 non-null  object \n",
            " 13  urls             16538 non-null  object \n",
            " 14  photos           16538 non-null  object \n",
            " 15  replies_count    16538 non-null  float64\n",
            " 16  retweets_count   16538 non-null  float64\n",
            " 17  likes_count      16538 non-null  float64\n",
            " 18  hashtags         16538 non-null  object \n",
            " 19  cashtags         16538 non-null  object \n",
            " 20  link             16538 non-null  object \n",
            " 21  retweet          16538 non-null  object \n",
            " 22  quote_url        1408 non-null   object \n",
            " 23  video            16538 non-null  float64\n",
            " 24  thumbnail        353 non-null    object \n",
            " 25  near             0 non-null      float64\n",
            " 26  geo              0 non-null      float64\n",
            " 27  source           0 non-null      float64\n",
            " 28  user_rt_id       0 non-null      float64\n",
            " 29  user_rt          0 non-null      float64\n",
            " 30  retweet_id       0 non-null      float64\n",
            " 31  reply_to         16538 non-null  object \n",
            " 32  retweet_date     0 non-null      float64\n",
            " 33  translate        0 non-null      float64\n",
            " 34  trans_src        0 non-null      float64\n",
            " 35  trans_dest       0 non-null      float64\n",
            "dtypes: float64(18), object(18)\n",
            "memory usage: 4.5+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrpSkY-0mcUp"
      },
      "source": [
        "# make a copy if you need so that the changes made in original df doesn't affect the copy\n",
        "df_copy = df.copy(deep=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhN4Kc1cXWmc"
      },
      "source": [
        "# I don't need these columns, so dropping them. You can keep them if you want.\n",
        "drop_list = ['place','near','geo','source','user_rt_id','user_rt','retweet_id','retweet_date','translate','trans_src','trans_dest']\n",
        "df = df.drop(columns=drop_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao5-gPbiTMHT"
      },
      "source": [
        "# remove URLs\n",
        "df['tweet'] = df['tweet'].str.replace('http\\S+|www.\\S+', '',case=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a00s0Px1_6w5"
      },
      "source": [
        "#hide\n",
        "#!pip install texthero\n",
        "import texthero as hero"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I6QFZNtSudX"
      },
      "source": [
        "# text preprocessing\n",
        "from texthero import preprocessing\n",
        "\n",
        "# create a custom pipeline to preprocess the raw text we have\n",
        "custom_pipeline = [preprocessing.fillna\n",
        "                   , preprocessing.lowercase\n",
        "                   , preprocessing.remove_digits\n",
        "                   , preprocessing.remove_punctuation\n",
        "                   , preprocessing.remove_diacritics\n",
        "                   , preprocessing.remove_stopwords]\n",
        "                  #  , preprocessing.remove_whitespace\n",
        "                  #  , preprocessing.stem]\n",
        "\n",
        "# call clean() method to clean the raw text in 'tweet' col and pass the custom_pipeline to pipeline argument\n",
        "df['clean_tweet'] = hero.clean(df['tweet'], pipeline = custom_pipeline)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_kVRHbiyd4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "2810408e-5884-4d65-f6eb-7a7fbe93802f"
      },
      "source": [
        "#hide\n",
        "!pip3 install sweetviz\n",
        "import sweetviz as sv"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sweetviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/06/f7341e6dc3fae77962855001cd1c1a6a73e3f094ffba2039b4dafe66c751/sweetviz-2.1.2-py3-none-any.whl (15.1MB)\n",
            "\u001b[K     |████████████████████████████████| 15.1MB 200kB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (2.11.3)\n",
            "Collecting tqdm>=4.43.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.4.1)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.19.5)\n",
            "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (5.1.4)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->sweetviz) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2.8.1)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.2.0->sweetviz) (3.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (1.15.0)\n",
            "Installing collected packages: tqdm, sweetviz\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed sweetviz-2.1.2 tqdm-4.61.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lal7MxsKIc_S"
      },
      "source": [
        "# Visualizations using TextHero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFHSI4KNZL79"
      },
      "source": [
        "#df1 = df.drop(columns=['tweet','username','link'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IKuqNM57SCv"
      },
      "source": [
        "df['pca'] = (\n",
        "            df['clean_tweet']\n",
        "            .pipe(hero.clean)\n",
        "            .pipe(hero.tfidf)\n",
        "            .pipe(hero.pca)\n",
        "   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HKg982s8PhZ"
      },
      "source": [
        "df['pca'] = hero.pca(df['tfidf'])\n",
        "hero.scatterplot(\n",
        "    df, \n",
        "    col='pca', \n",
        "    color='topic', \n",
        "    title=\"PCA Woj & Shams\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4z9fS6BZo4_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# using top_words() method, get the top N words and make a bar plot.\n",
        "hero.top_words(df1['clean_tweet']).head(20).plot.bar(figsize=(15,10))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27l3QrzZ1tU"
      },
      "source": [
        "# Want to add more stop words to your list? No problem. Follow the below steps.\n",
        "\n",
        "from texthero import stopwords\n",
        "default_stopwords = stopwords.DEFAULT\n",
        "#add a list of stopwords to the stopwords\n",
        "stop_w = [\"co\",\"https\",\"http\", \"tell\", \"tells\", \"game\", \"season\", \"sports\", \"two\"]\n",
        "custom_stopwords = default_stopwords.union(set(stop_w))\n",
        "#Call remove_stopwords and pass the custom_stopwords list\n",
        "df1['clean_tweet'] = hero.remove_stopwords(df1['clean_tweet'], custom_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvaWXQM4W_u1"
      },
      "source": [
        "# Let's visualize again.\n",
        "\n",
        "hero.top_words(df1['clean_tweet']).head(20).plot.bar(figsize=(15,10))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w-PqD_RbSUz"
      },
      "source": [
        "# just checking for any null values\n",
        "df1.clean_tweet.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuWr9Y_6TpdF"
      },
      "source": [
        "# WordCloud with single line of code.\n",
        "\n",
        "hero.visualization.wordcloud(df1['clean_tweet'],width = 400, height= 400,background_color='White')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plo0brZQdr0j"
      },
      "source": [
        "#Add pca value to dataframe to use as visualization coordinates\n",
        "df1['pca'] = (\n",
        "            df1['clean_tweet']\n",
        "            .pipe(hero.tfidf,max_features=300)\n",
        "            .pipe(hero.pca)\n",
        "   )\n",
        "#Add k-means cluster to dataframe \n",
        "df1['kmeans'] = (\n",
        "            df1['clean_tweet']\n",
        "            .pipe(hero.tfidf,max_features=300)\n",
        "            .pipe(hero.kmeans, n_clusters=5)\n",
        "   )\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1eC-aJEe27h"
      },
      "source": [
        "# Generate scatter plot for pca and kmeans. Cool isn't it?\n",
        "hero.scatterplot(df1, 'pca', color = 'kmeans', hover_data=['clean_tweet'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e1qAnyrTk4z"
      },
      "source": [
        "# 7. Other Visualizations for further analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLf9aPnZVs_w"
      },
      "source": [
        "#hide\n",
        "!pip3 install chart-studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyuLXsr9UOOz"
      },
      "source": [
        "#hide\n",
        "import seaborn as sns # visualization library\n",
        "import chart_studio.plotly as py # visualization library\n",
        "from plotly.offline import init_notebook_mode, iplot # plotly offline mode\n",
        "init_notebook_mode(connected=True) \n",
        "import plotly.graph_objs as go # plotly graphical object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v39hAZoRWEuU"
      },
      "source": [
        "df2 = df.drop(columns=['username','tweet','link'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr4tuH4GWE3C"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3aIWLLyWFCd"
      },
      "source": [
        "plt.figure(figsize=(17,10))\n",
        "sns.lineplot(data=df2['retweets_count'], dashes=False)\n",
        "plt.title(\"Retweets over time\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmjXNbUdWFFz"
      },
      "source": [
        "plt.figure(figsize=(17,10))\n",
        "sns.lineplot(data=df2['replies_count'], dashes=False)\n",
        "plt.title(\"Replies over time\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0AWNAgjb3w7"
      },
      "source": [
        "plt.figure(figsize=(17,10))\n",
        "sns.lineplot(data=df2['likes_count'], dashes=False)\n",
        "plt.title(\"Likes over time\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2P21VHifkaR"
      },
      "source": [
        "data_count = data_count[:20,]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(data_count.values, data_count.index, alpha=0.8)\n",
        "plt.title(‘Top Words Overall’)\n",
        "plt.ylabel(‘Word from Tweet’, fontsize=12)\n",
        "plt.xlabel(‘Count of Words’, fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2_PylJBjIWP"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "temp=' '.join(df['cleaned_tweets'].tolist())\n",
        "wordcloud = WordCloud(width = 800, height = 500, \n",
        "                background_color ='white', \n",
        "                min_font_size = 10).generate(temp)\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVlykRVPjJax"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "def plot_topn(sentences, ngram_range=(1,3), top=20,firstword=''):\n",
        "    c=CountVectorizer(ngram_range=ngram_range)\n",
        "    X=c.fit_transform(sentences)\n",
        "    words=pd.DataFrame(X.sum(axis=0),columns=c.get_feature_names()).T.sort_values(0,ascending=False).reset_index()\n",
        "    res=words[words['index'].apply(lambda x: firstword in x)].head(top)\n",
        "    pl=px.bar(res, x='index',y=0)\n",
        "    pl.update_layout(yaxis_title='count',xaxis_title='Phrases')\n",
        "    pl.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5X_KmPbjP71"
      },
      "source": [
        "plot_topn(tweet_list, ngram_range=(1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVPgwDOOjULF"
      },
      "source": [
        "plot_topn(tweet_list, ngram_range=(2,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9siCvpLgjXJ4"
      },
      "source": [
        "plot_topn(tweet_list, ngram_range=(3,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdSdA0Rrjd1b"
      },
      "source": [
        "from textblob import TextBlob\n",
        "df['sentiment']=df['tweet'].apply(lambda x:TextBlob(x).sentiment[0])\n",
        "df['subject']=df['tweet'].apply(lambda x: TextBlob(x).sentiment[1])\n",
        "df['polarity']=df['sentiment'].apply(lambda x: 'pos' if x>=0 else 'neg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKlHmEvnjezo"
      },
      "source": [
        "fig=px.histogram(df[df['subject']>0.5], x='polarity', color='polarity')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGzJ85Pxjkm4"
      },
      "source": [
        "#pre-process tweets to BOW\n",
        "from gensim import corpora\n",
        "r = [process_text(x,stem=False).split() for x in df['tweet'].tolist()] \n",
        "dictionary = corpora.Dictionary(r)\n",
        "corpus = [dictionary.doc2bow(rev) for rev in r]\n",
        "#initialize model and print topics\n",
        "from gensim import models\n",
        "model = models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
        "topics = model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topics[0],process_text(topic[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyv_z2QUjlt3"
      },
      "source": [
        "labels=[]\n",
        "for x in model[corpus]:\n",
        "    labels.append(sorted(x,key=lambda x: x[1],reverse=True)[0][0])\n",
        "df['topic']=pd.Series(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mvDBzcCUvlM"
      },
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1j4_tqWRk5"
      },
      "source": [
        "#hide\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nwnu-AoXj6y"
      },
      "source": [
        "data_count = data_count[:20,]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(data_count.values, data_count.index, alpha=0.8)\n",
        "plt.title(‘Top Words Overall’)\n",
        "plt.ylabel(‘Word from Tweet’, fontsize=12)\n",
        "plt.xlabel(‘Count of Words’, fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcOx4_HUkCin"
      },
      "source": [
        "# import twint\n",
        "# # Set up TWINT config\n",
        "# c = twint.Config()\n",
        "# c.Search = \"Oneplus 9 pro\"\n",
        "# # Custom output format\n",
        "# c.Limit = 3000\n",
        "# c.Pandas = True\n",
        "# twint.run.Search(c)\n",
        "\n",
        "def column_names():\n",
        "    return twint.output.panda.Tweets_df.columns\n",
        "def twint_to_pd(columns):\n",
        "    return twint.output.panda.Tweets_df[columns]\n",
        "\n",
        "column_names()\n",
        "tweet_df = twint_to_pd([\"date\", \"username\", \"tweet\", \"hashtags\", \"likes_count\"])\n",
        "tweet_df.head(10)\n",
        "\n",
        "print(len(tweet_df))\n",
        "\n",
        "from transformers import pipeline\n",
        "sentiment_classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "results = sentiment_classifier(tweet_df['tweet'].tolist())\n",
        "\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3DfsXEBkDlS"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re  \n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTgLtGDhkZCA"
      },
      "source": [
        "\n",
        "from twitterscraper import query_tweets\n",
        "from twitterscraper.query import query_tweets_from_user\n",
        "import datetime as dt \n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "begin_date = dt.date(2020,7,1)\n",
        "end_date = dt.date(2020,7,13)\n",
        "\n",
        "\n",
        "limit = 100\n",
        "lang = 'english'\n",
        "\n",
        "#Use this to search a specific user\n",
        "\n",
        "user = 'realDonaldTrump'\n",
        "tweets = query_tweets_from_user(user)\n",
        "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
        "\n",
        "df = df.loc[df['screen_name'] == user]\n",
        "\n",
        "df = df['text']\n",
        "\n",
        "df\n",
        "\n",
        "#Use this if wanting to seach for a specific Phrase or word\n",
        "\n",
        "#tweets = query_tweets('impeachment', begindate = begin_date, enddate = end_date, limit = limit, lang = lang)\n",
        "#df = pd.DataFrame(t.__dict__ for t in tweets)\n",
        "\n",
        "#df = df['text']\n",
        "\n",
        "#df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQfzEAgkezG"
      },
      "source": [
        "#This splits all the sentences up which makes it easier for us to work with\n",
        "\n",
        "all_sentences = []\n",
        "\n",
        "for word in df:\n",
        "    all_sentences.append(word)\n",
        "\n",
        "all_sentences\n",
        "#df1 = df.to_string()\n",
        "\n",
        "#df_split = df1.split()\n",
        "\n",
        "#df_split\n",
        "lines = list()\n",
        "for line in all_sentences:    \n",
        "    words = line.split()\n",
        "    for w in words: \n",
        "       lines.append(w)\n",
        "\n",
        "\n",
        "print(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HznXsTLAkn5s"
      },
      "source": [
        "#Removing Punctuation\n",
        "\n",
        "lines = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in lines]\n",
        "\n",
        "lines\n",
        "\n",
        "lines2 = []\n",
        "\n",
        "for word in lines:\n",
        "    if word != '':\n",
        "        lines2.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZeAqCfykpvQ"
      },
      "source": [
        "#This is stemming the words to their root\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# The Snowball Stemmer requires that you pass a language parameter\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "stem = []\n",
        "for word in lines2:\n",
        "    stem.append(s_stemmer.stem(word))\n",
        "    \n",
        "#stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ741gpHkzwj"
      },
      "source": [
        "#Removing all Stop Words\n",
        "\n",
        "stem2 = []\n",
        "\n",
        "for word in stem:\n",
        "    if word not in nlp.Defaults.stop_words:\n",
        "        stem2.append(word)\n",
        "\n",
        "#stem2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rlHOLYdk22c"
      },
      "source": [
        "df = pd.DataFrame(stem2)\n",
        "\n",
        "df = df[0].value_counts()\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "freqdoctor = FreqDist()\n",
        "\n",
        "for words in df:\n",
        "    freqdoctor[words] += 1\n",
        "\n",
        "freqdoctor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-BKloDBlB-c"
      },
      "source": [
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1TcyJE2lGtj"
      },
      "source": [
        "df = df[:20,]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(df.values, df.index, alpha=0.8)\n",
        "plt.title('Top Words Overall')\n",
        "plt.ylabel('Word from Tweet', fontsize=12)\n",
        "plt.xlabel('Count of Words', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l4hbQcnlH1P"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfQql20RlNxv"
      },
      "source": [
        "def show_ents(doc):\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgzlkT1lRfd"
      },
      "source": [
        "str1 = \" \" \n",
        "stem2 = str1.join(lines2)\n",
        "\n",
        "stem2 = nlp(stem2)\n",
        "\n",
        "label = [(X.text, X.label_) for X in stem2.ents]\n",
        "\n",
        "df6 = pd.DataFrame(label, columns = ['Word','Entity'])\n",
        "\n",
        "df7 = df6.where(df6['Entity'] == 'ORG')\n",
        "\n",
        "df7 = df7['Word'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlNiWfszlSk2"
      },
      "source": [
        "df = df7[:20,]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(df.values, df.index, alpha=0.8)\n",
        "plt.title('Top Organizations Mentioned')\n",
        "plt.ylabel('Word from Tweet', fontsize=12)\n",
        "plt.xlabel('Count of Words', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFxcECIilWkN"
      },
      "source": [
        "str1 = \" \" \n",
        "stem2 = str1.join(lines2)\n",
        "\n",
        "stem2 = nlp(stem2)\n",
        "\n",
        "label = [(X.text, X.label_) for X in stem2.ents]\n",
        "\n",
        "df10 = pd.DataFrame(label, columns = ['Word','Entity'])\n",
        "\n",
        "df10 = df10.where(df10['Entity'] == 'PERSON')\n",
        "\n",
        "df11 = df10['Word'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj9LCTHblb_0"
      },
      "source": [
        "df = df11[:20,]\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(df.values, df.index, alpha=0.8)\n",
        "plt.title('Top People Mentioned')\n",
        "plt.ylabel('Word from Tweet', fontsize=12)\n",
        "plt.xlabel('Count of Words', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPPoKc_1UvlM"
      },
      "source": [
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmz90xA2UvlN"
      },
      "source": [
        "txt = files[0].open().read(); txt[:75]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pctKsXERUvlN"
      },
      "source": [
        "spacy = WordTokenizer()\n",
        "toks = first(spacy([txt]))\n",
        "print(coll_repr(toks, 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCTdPzYTUvlO"
      },
      "source": [
        "first(spacy(['The U.S. dollar $1 is $1.00.']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd9g_kNwUvlO"
      },
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_q8ugPJUvlO"
      },
      "source": [
        "defaults.text_proc_rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc9nkoyAUvlO"
      },
      "source": [
        "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn_C1TV6UvlP"
      },
      "source": [
        "### Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVY3hvz4UvlP"
      },
      "source": [
        "txts = L(o.open().read() for o in files[:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKSzqj3tUvlP"
      },
      "source": [
        "def subword(sz):\n",
        "    sp = SubwordTokenizer(vocab_sz=sz)\n",
        "    sp.setup(txts)\n",
        "    return ' '.join(first(sp([txt]))[:40])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qglExAd_UvlP"
      },
      "source": [
        "subword(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNMDQwGsUvlP"
      },
      "source": [
        "subword(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAoCea0iUvlQ"
      },
      "source": [
        "subword(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtiqvtFcUvlQ"
      },
      "source": [
        "### Numericalization with fastai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdjr6khOUvlQ"
      },
      "source": [
        "toks = tkn(txt)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruh17YcOUvlQ"
      },
      "source": [
        "toks200 = txts[:200].map(tkn)\n",
        "toks200[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snWnNbhkUvlR"
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)\n",
        "coll_repr(num.vocab,20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7THdA7EUvlR"
      },
      "source": [
        "nums = num(toks)[:20]; nums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqNHKTahUvlR"
      },
      "source": [
        "' '.join(num.vocab[o] for o in nums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-fqNunUvlR"
      },
      "source": [
        "### Putting Our Texts into Batches for a Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ghKT4uUvlR"
      },
      "source": [
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "tokens = tkn(stream)\n",
        "bs,seq_len = 6,15\n",
        "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCQ6xcWtUvlR"
      },
      "source": [
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg92hzqEUvlS"
      },
      "source": [
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB-3_KjYUvlS"
      },
      "source": [
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah7OObZgUvlS"
      },
      "source": [
        "nums200 = toks200.map(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH4Zl5NFUvlS"
      },
      "source": [
        "dl = LMDataLoader(nums200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfmhOsuUvlS"
      },
      "source": [
        "x,y = first(dl)\n",
        "x.shape,y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJRyRx9AUvlS"
      },
      "source": [
        "' '.join(num.vocab[o] for o in x[0][:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmD3rJCiUvlS"
      },
      "source": [
        "' '.join(num.vocab[o] for o in y[0][:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6npKMA_UvlT"
      },
      "source": [
        "## Training a Text Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C-NWzFmUvlT"
      },
      "source": [
        "### Language Model Using DataBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR8LizGMUvlT"
      },
      "source": [
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
        "\n",
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ").dataloaders(path, path=path, bs=128, seq_len=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Lfji3UUvlT"
      },
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_FliBZYUvlT"
      },
      "source": [
        "### Fine-Tuning the Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovPDL4FtUvlT"
      },
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR4YyLMyUvlT"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kokn7emDUvlT"
      },
      "source": [
        "### Saving and Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp3yHKdTUvlT"
      },
      "source": [
        "learn.save('1epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZCtXe1uUvlU"
      },
      "source": [
        "learn = learn.load('1epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM5nuyp1UvlU"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, 2e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo3o7dP_UvlU"
      },
      "source": [
        "learn.save_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMdRN0uqUvlU"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4z0VqFbUvlU"
      },
      "source": [
        "TEXT = \"Breaking news: LeBron James has a \"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
        "         for _ in range(N_SENTENCES)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nds5XkLVUvlU"
      },
      "source": [
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qikxU4xfUvlU"
      },
      "source": [
        "### Creating the Classifier DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u96iiShxUvlU"
      },
      "source": [
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhcIpdonUvlU"
      },
      "source": [
        "dls_clas.show_batch(max_n=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJwazgnYUvlU"
      },
      "source": [
        "nums_samp = toks200[:10].map(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_0LYvu_UvlV"
      },
      "source": [
        "nums_samp.map(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSD0CX7JUvlV"
      },
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
        "                                metrics=accuracy).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z84kWWuRUvlV"
      },
      "source": [
        "learn = learn.load_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGiznYKzUvlV"
      },
      "source": [
        "### Fine-Tuning the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8pEvZ77UvlV"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idrt6dSgUvlV"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNS9-4ezUvlV"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hORJe4cUvlV"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}